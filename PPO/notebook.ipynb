{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import flax.linen as nn\n",
    "import flax\n",
    "import gymnasium as gym\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "class GenericPolicy(nn.Module):\n",
    "    state_dim: int\n",
    "    n_actions: int = 4\n",
    "    hidden_dim: int = 64\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, state):\n",
    "        x = nn.Dense(self.hidden_dim)(state)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(self.n_actions)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ValueFunction(nn.Module):\n",
    "    state_dim: int\n",
    "    hidden_dim: int = 64\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, state):\n",
    "        x = nn.Dense(self.hidden_dim)(state)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(1)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def policy_loss(params, policy, logits, states, actions, advantages, eps=0.3):\n",
    "    current_logits = nn.log_softmax(policy.apply(params, states))\n",
    "    indexed_logits = jnp.take_along_axis(current_logits, jnp.expand_dims(actions, 1), axis=1)\n",
    "    ratios = jnp.exp(indexed_logits - logits)\n",
    "    unclipped_objective = ratios * advantages\n",
    "    clipped_objective = jnp.where(advantages >= 0, \n",
    "                                  (1 + eps) * advantages, \n",
    "                                  (1 - eps) * advantages)\n",
    "    loss = -jnp.mean(jnp.minimum(unclipped_objective, clipped_objective))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def value_function_loss( params, value_fn, state, reward):\n",
    "    advantages = jnp.expand_dims(reward,1) - value_fn.apply(params, state)\n",
    "    return jnp.mean(advantages**2), advantages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Average Episode Reward: 26.0\n",
      "Step: 0, Policy Loss: -0.8916085958480835, Value Loss: 0.8428187966346741\n",
      "Step: 1, Average Episode Reward: 17.9\n",
      "Step: 1, Policy Loss: -0.8510328531265259, Value Loss: 0.7779422998428345\n",
      "Step: 2, Average Episode Reward: 22.0\n",
      "Step: 2, Policy Loss: -0.8632423877716064, Value Loss: 0.7820778489112854\n",
      "Step: 3, Average Episode Reward: 25.5\n",
      "Step: 3, Policy Loss: -0.8581407070159912, Value Loss: 0.7659480571746826\n",
      "Step: 4, Average Episode Reward: 30.5\n",
      "Step: 4, Policy Loss: -0.834945023059845, Value Loss: 0.7442376613616943\n",
      "Step: 5, Average Episode Reward: 23.3\n",
      "Step: 5, Policy Loss: -0.83938068151474, Value Loss: 0.7518985867500305\n",
      "Step: 6, Average Episode Reward: 32.5\n",
      "Step: 6, Policy Loss: -0.791509747505188, Value Loss: 0.6944068074226379\n",
      "Step: 7, Average Episode Reward: 27.0\n",
      "Step: 7, Policy Loss: -0.7432982921600342, Value Loss: 0.6264093518257141\n",
      "Step: 8, Average Episode Reward: 20.1\n",
      "Step: 8, Policy Loss: -0.7705106139183044, Value Loss: 0.6543177366256714\n",
      "Step: 9, Average Episode Reward: 21.5\n",
      "Step: 9, Policy Loss: -0.6802763938903809, Value Loss: 0.5454532504081726\n",
      "Step: 10, Average Episode Reward: 26.2\n",
      "Step: 10, Policy Loss: -0.652380645275116, Value Loss: 0.5269262194633484\n",
      "Step: 11, Average Episode Reward: 24.7\n",
      "Step: 11, Policy Loss: -0.6156123280525208, Value Loss: 0.5190240740776062\n",
      "Step: 12, Average Episode Reward: 25.3\n",
      "Step: 12, Policy Loss: -0.7417351603507996, Value Loss: 0.5966976881027222\n",
      "Step: 13, Average Episode Reward: 30.7\n",
      "Step: 13, Policy Loss: -0.667778730392456, Value Loss: 0.5468899011611938\n",
      "Step: 14, Average Episode Reward: 29.7\n",
      "Step: 14, Policy Loss: -0.7805688977241516, Value Loss: 0.7271029353141785\n",
      "Step: 15, Average Episode Reward: 24.8\n",
      "Step: 15, Policy Loss: -0.7201149463653564, Value Loss: 0.5706915259361267\n",
      "Step: 16, Average Episode Reward: 25.7\n",
      "Step: 16, Policy Loss: -0.6417812705039978, Value Loss: 0.5513768196105957\n",
      "Step: 17, Average Episode Reward: 28.5\n",
      "Step: 17, Policy Loss: -0.6785568594932556, Value Loss: 0.5443168878555298\n",
      "Step: 18, Average Episode Reward: 23.8\n",
      "Step: 18, Policy Loss: -0.5688620805740356, Value Loss: 0.5223493576049805\n",
      "Step: 19, Average Episode Reward: 27.8\n",
      "Step: 19, Policy Loss: -0.5651469826698303, Value Loss: 0.4830159842967987\n",
      "Step: 20, Average Episode Reward: 29.5\n",
      "Step: 20, Policy Loss: -0.5941272974014282, Value Loss: 0.4830070734024048\n",
      "Step: 21, Average Episode Reward: 29.8\n",
      "Step: 21, Policy Loss: -0.5563861131668091, Value Loss: 0.4406874477863312\n",
      "Step: 22, Average Episode Reward: 24.1\n",
      "Step: 22, Policy Loss: -0.4978064298629761, Value Loss: 0.43903470039367676\n",
      "Step: 23, Average Episode Reward: 20.9\n",
      "Step: 23, Policy Loss: -0.5332307815551758, Value Loss: 0.43443208932876587\n",
      "Step: 24, Average Episode Reward: 23.6\n",
      "Step: 24, Policy Loss: -0.4791235327720642, Value Loss: 0.3717489540576935\n",
      "Step: 25, Average Episode Reward: 27.4\n",
      "Step: 25, Policy Loss: -0.5375080108642578, Value Loss: 0.4320897161960602\n",
      "Step: 26, Average Episode Reward: 26.6\n",
      "Step: 26, Policy Loss: -0.3848400413990021, Value Loss: 0.38916924595832825\n",
      "Step: 27, Average Episode Reward: 24.1\n",
      "Step: 27, Policy Loss: -0.4395298659801483, Value Loss: 0.46129778027534485\n",
      "Step: 28, Average Episode Reward: 30.2\n",
      "Step: 28, Policy Loss: -0.49578428268432617, Value Loss: 0.41375070810317993\n",
      "Step: 29, Average Episode Reward: 25.0\n",
      "Step: 29, Policy Loss: -0.3411087691783905, Value Loss: 0.29472577571868896\n",
      "Step: 30, Average Episode Reward: 28.6\n",
      "Step: 30, Policy Loss: -0.3213273584842682, Value Loss: 0.4208972454071045\n",
      "Step: 31, Average Episode Reward: 26.8\n",
      "Step: 31, Policy Loss: -0.4424664378166199, Value Loss: 0.368095338344574\n",
      "Step: 32, Average Episode Reward: 19.4\n",
      "Step: 32, Policy Loss: -0.4243294596672058, Value Loss: 0.35532402992248535\n",
      "Step: 33, Average Episode Reward: 25.8\n",
      "Step: 33, Policy Loss: -0.4654836654663086, Value Loss: 0.40303659439086914\n",
      "Step: 34, Average Episode Reward: 26.1\n",
      "Step: 34, Policy Loss: -0.4412374794483185, Value Loss: 0.33078235387802124\n",
      "Step: 35, Average Episode Reward: 27.1\n",
      "Step: 35, Policy Loss: -0.36047160625457764, Value Loss: 0.34136393666267395\n",
      "Step: 36, Average Episode Reward: 22.8\n",
      "Step: 36, Policy Loss: -0.4960302412509918, Value Loss: 0.3970988988876343\n",
      "Step: 37, Average Episode Reward: 22.4\n",
      "Step: 37, Policy Loss: -0.3319321870803833, Value Loss: 0.3091326653957367\n",
      "Step: 38, Average Episode Reward: 29.9\n",
      "Step: 38, Policy Loss: -0.43150976300239563, Value Loss: 0.32673659920692444\n",
      "Step: 39, Average Episode Reward: 24.0\n",
      "Step: 39, Policy Loss: -0.39027369022369385, Value Loss: 0.3310994505882263\n",
      "Step: 40, Average Episode Reward: 22.8\n",
      "Step: 40, Policy Loss: -0.49135154485702515, Value Loss: 0.373086154460907\n",
      "Step: 41, Average Episode Reward: 25.4\n",
      "Step: 41, Policy Loss: -0.3587276339530945, Value Loss: 0.3331674039363861\n",
      "Step: 42, Average Episode Reward: 34.1\n",
      "Step: 42, Policy Loss: -0.45414602756500244, Value Loss: 0.3489116430282593\n",
      "Step: 43, Average Episode Reward: 23.5\n",
      "Step: 43, Policy Loss: -0.4906140863895416, Value Loss: 0.35316139459609985\n",
      "Step: 44, Average Episode Reward: 33.2\n",
      "Step: 44, Policy Loss: -0.4634862244129181, Value Loss: 0.34967750310897827\n",
      "Step: 45, Average Episode Reward: 24.8\n",
      "Step: 45, Policy Loss: -0.36570268869400024, Value Loss: 0.2708281874656677\n",
      "Step: 46, Average Episode Reward: 22.9\n",
      "Step: 46, Policy Loss: -0.3897846043109894, Value Loss: 0.29991671442985535\n",
      "Step: 47, Average Episode Reward: 25.2\n",
      "Step: 47, Policy Loss: -0.33183789253234863, Value Loss: 0.28871408104896545\n",
      "Step: 48, Average Episode Reward: 26.9\n",
      "Step: 48, Policy Loss: -0.43759915232658386, Value Loss: 0.2763194143772125\n",
      "Step: 49, Average Episode Reward: 20.0\n",
      "Step: 49, Policy Loss: -0.3679503798484802, Value Loss: 0.2788340449333191\n",
      "Step: 50, Average Episode Reward: 24.3\n",
      "Step: 50, Policy Loss: -0.3631362020969391, Value Loss: 0.28461530804634094\n",
      "Step: 51, Average Episode Reward: 34.1\n",
      "Step: 51, Policy Loss: -0.44284212589263916, Value Loss: 0.2845342755317688\n",
      "Step: 52, Average Episode Reward: 46.6\n",
      "Step: 52, Policy Loss: -0.4224795997142792, Value Loss: 0.250264972448349\n",
      "Step: 53, Average Episode Reward: 30.5\n",
      "Step: 53, Policy Loss: -0.35292792320251465, Value Loss: 0.22524739801883698\n",
      "Step: 54, Average Episode Reward: 25.9\n",
      "Step: 54, Policy Loss: -0.38196107745170593, Value Loss: 0.26177719235420227\n",
      "Step: 55, Average Episode Reward: 20.6\n",
      "Step: 55, Policy Loss: -0.35756823420524597, Value Loss: 0.24496899545192719\n",
      "Step: 56, Average Episode Reward: 18.8\n",
      "Step: 56, Policy Loss: -0.27887192368507385, Value Loss: 0.2095831334590912\n",
      "Step: 57, Average Episode Reward: 27.1\n",
      "Step: 57, Policy Loss: -0.41667696833610535, Value Loss: 0.25321587920188904\n",
      "Step: 58, Average Episode Reward: 19.4\n",
      "Step: 58, Policy Loss: -0.32260823249816895, Value Loss: 0.22724029421806335\n",
      "Step: 59, Average Episode Reward: 21.2\n",
      "Step: 59, Policy Loss: -0.24975073337554932, Value Loss: 0.2296750843524933\n",
      "Step: 60, Average Episode Reward: 30.8\n",
      "Step: 60, Policy Loss: -0.39739489555358887, Value Loss: 0.23358386754989624\n",
      "Step: 61, Average Episode Reward: 34.5\n",
      "Step: 61, Policy Loss: -0.2817492187023163, Value Loss: 0.22770926356315613\n",
      "Step: 62, Average Episode Reward: 23.4\n",
      "Step: 62, Policy Loss: -0.2957491874694824, Value Loss: 0.205113485455513\n",
      "Step: 63, Average Episode Reward: 18.8\n",
      "Step: 63, Policy Loss: -0.2527683675289154, Value Loss: 0.17591330409049988\n",
      "Step: 64, Average Episode Reward: 18.9\n",
      "Step: 64, Policy Loss: -0.25186440348625183, Value Loss: 0.2107447385787964\n",
      "Step: 65, Average Episode Reward: 23.2\n",
      "Step: 65, Policy Loss: -0.3072275221347809, Value Loss: 0.2120257019996643\n"
     ]
    }
   ],
   "source": [
    "env = \"CartPole-v1\" \n",
    "train_steps=100\n",
    "lr=1e-3\n",
    "max_ep_len=100\n",
    "n_rollouts=10\n",
    "df=0.99\n",
    "\n",
    "env = gym.make(env)\n",
    "# initialize the model and optimizer\n",
    "policy = GenericPolicy(env.observation_space.shape[0], env.action_space.n)\n",
    "value_fn = ValueFunction(env.observation_space.shape[0])\n",
    "policy_optimizer = optax.adam(learning_rate=lr)\n",
    "value_optimizer = optax.adam(learning_rate=lr)\n",
    "\n",
    "policy_params = policy.init(\n",
    "    jax.random.PRNGKey(0), jax.numpy.zeros((1, env.observation_space.shape[0]))\n",
    ")\n",
    "\n",
    "value_params = value_fn.init(\n",
    "    jax.random.PRNGKey(0), jax.numpy.zeros((1, env.observation_space.shape[0]))\n",
    ")\n",
    "policy_optimizer_state = policy_optimizer.init(policy_params)\n",
    "value_optimizer_state = value_optimizer.init(value_params)\n",
    "\n",
    "value_grad_fn = jax.value_and_grad(value_function_loss, has_aux=True)\n",
    "policy_grad_fn = jax.value_and_grad(policy_loss)\n",
    "key = jax.random.PRNGKey(0)\n",
    "# create gym environment\n",
    "for step in range(train_steps):\n",
    "    # collect data\n",
    "    data = []\n",
    "    total_reward = 0\n",
    "    for _ in range(n_rollouts):\n",
    "        ep = []\n",
    "        state,_ = env.reset()\n",
    "        state = jnp.array(state)\n",
    "        for _ in range(max_ep_len):\n",
    "            logits = nn.log_softmax(policy.apply(policy_params, state))\n",
    "            key, subkey = jax.random.split(key)\n",
    "            action = jax.random.categorical(subkey, logits)\n",
    "            action_prob = logits[action]\n",
    "            next_state, reward, done, _,_ = env.step(action.item())\n",
    "            next_state = jnp.array(next_state)\n",
    "            # convert above line to dict\n",
    "            total_reward += reward\n",
    "            ep.append(\n",
    "                {\n",
    "                    \"state\": state,\n",
    "                    \"action\": action,\n",
    "                    \"action_prob\": action_prob,\n",
    "                    \"reward\": reward,\n",
    "                    \"next_state\": next_state,\n",
    "                    \"done\": done,\n",
    "                }\n",
    "            )\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        # use discounting to attribute rewards for episodes\n",
    "        for tm1, t in zip(ep[:-1][:-1:-1], ep[1:][::-1]):\n",
    "            tm1[\"reward\"] += df * t[\"reward\"]\n",
    "        data.extend(ep)\n",
    "    print(f\"Step: {step}, Average Episode Reward: {total_reward/n_rollouts}\")\n",
    "    # batch, compute advantage, and update value function\n",
    "    states = jax.numpy.array([d[\"state\"] for d in data])\n",
    "    rewards = jax.numpy.array([d[\"reward\"] for d in data])\n",
    "    actions = jax.numpy.array([d[\"action\"] for d in data])\n",
    "    logits = jax.numpy.array([d[\"action_prob\"] for d in data])\n",
    "    # call jax value and grad to get advantadges and gradients+loss\n",
    "    (value_loss, advantadges), value_grads = value_grad_fn(\n",
    "        value_params, value_fn, states, rewards\n",
    "    )\n",
    "\n",
    "    # apply gradients to value function\n",
    "    value_updates, value_optimizer_state = value_optimizer.update(value_grads, value_optimizer_state)\n",
    "    value_params = optax.apply_updates(value_params, value_updates)\n",
    "\n",
    "    # update policy\n",
    "    policy_loss, policy_grads = policy_grad_fn(\n",
    "        policy_params, policy, logits, states, actions, advantadges\n",
    "    )\n",
    "    policy_updates, policy_optimizer_state = policy_optimizer.update(policy_grads, policy_optimizer_state)\n",
    "    policy_params = optax.apply_updates(policy_params, policy_updates)\n",
    "    print(f\"Step: {step}, Policy Loss: {policy_loss}, Value Loss: {value_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 179)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advantadges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(2, dtype=int32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([2718843009, 1272950319], dtype=uint32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subkey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-1.473531 , -1.5424082, -1.3628829, -1.2003208], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algorithms-pxb4UjS2-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
